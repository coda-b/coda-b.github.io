---
layout: post
title: "The universe information capacity"
description: "We are an impossibility in an impossible universe (Ray Bradbury)"
op_image: "op-image.jpg"
tags: [universe, physics, qm]
---

>   “Ask anybody what the physical world is made of, and you are likely to be told “matter and energy.”  Yet if we have learned anything from engineering, biology and physics, information is just as crucial an ingredient.  ... a century of developments in physics has taught us that information is a crucial player in physical systems and processes.  Indeed, a current trend, initiated by John A. Wheeler of Princeton University, is to regard the physical world as made of information, with energy and matter as incidentals. (Bekenstein, 2003)

Professor Bekenstein regards “information” as a third force underlying the manifestations of matter and energy within time and space.  This is similar to Bohm’s ‘pilot waves’ and the ‘quantum potential’ permeating space, as information fields of the implicate orders that inform transformations of matter.

## Black Holes
______
> Theoretical results about black holes suggest that the universe could be like a gigantic hologram (Dr. Bekenstein)

I'll begin with the definition. “The Universe is all of time and space and its contents. It includes planets, moons, minor planets, stars, galaxies, the contents of intergalactic space, and all matter and energy“. 
Any galaxy, stars, quasars, minor planets, countries and continents and even your pointless existing are part of our galaxy.

To find out how huge our universe it can be digitized somehow. 
Here emerges the other question: How far we have to get to find and measure the minimal unit of information ‒ bit? 
Part of the observable universe which is available for study using modern astronomical/astrophysical methods is called Metagalaxy, but I'll use simplified term ‘observable universe‘ for the rest of this article.
Regardless on that the number of individual atoms in the observable universe is nearly equal to 10^81, does this mean that we need at least 10^81 bits of data to digitize it somehow? 
Is this enough? Atoms also consist a set of protons, electrons and neutrons. 
This means that I need much more bits for trying to digitize the universe.

How much information units can fit in a certain volume in space?

First I'll describe a direct, but not very obvious connection between information and entropy. 
Entropy is the interesting and ambiguous thing in modern physics. 
Although most of the laws, theorems and phenomena from impossibility of making the perpetual motion machine to definition of ‘time‘ per se is due to the definition of entropy. 
In 1877, physicist Ludwig Boltzman defined the term “thermodynamic entropy“ as the number of distinc microscopic states that particles in a body of matter could be in.
Entropy has lots of definitions, here I'm going to follow this one: 
> Entropy is a non-ordered system of chaos measure means that it's ultimately defined by an amount of different states which system can take

Suppose there is a system (baloon for instance) filled with some gas. 
The surface of the sphere is maximally elastic and has no frictional force or any limitation of the elastic force and then it depends on the amount of gas inside. 

{% include image.html path="images/universe_capacity/0.png" path-detail="images/universe_capacity/0.png" alt="" %}

Air molecules (basically nitrogen, oxygen, carbon dioxide and water) is in the uttermost chaos. And this means that the systems entropy is quite high.

Let's take these two identical systems and connect them using a tubule or something (the size of which is also can be neglected).

{% include image.html path="images/universe_capacity/1.png" path-detail="images/universe_capacity/1.png" alt="" %}

For quantitative evaluation of changing thermodynamic entropy there appears logarithms and Boltzmann constant (k). 
But in isothermal processes (where t:=const) we can reduce logarithms and rougly speaking say that entropy was increased 2 times.

In 1948, Claude Shannon defined entropy in terms of information theory. 
His theory depicts the number of binary bits needed to decode the information content within a message.
These two measures and concepts of entropy are “conceptually equivalent”, although they are expressed in different units—either ‘units of energy divided by temperature’ for thermal entropy, or as ‘bits’ which are “essentially dimensionless” for information entropy.
The conservation of information is also demanded within quantum mechanics.

According to the “Generalized Second Law of Thermodynamics“ the loss or gain of entropy in a material/energetic system must be compensated for by changes in the entropy of information - as one balances the other in order to maintain an overall GSL.

> When matter falls into a black hole, the increase in black hole entropy always compensates or overcompensates for the “lost” entropy of the matter. More generally, the sum of black hole entropies and the ordinary entropy outside the black holes cannot decrease.  This is the generalized second law—GSL for short. 

This implies that when the matter and energy of a quantum system are absorbed into a black hole, huge amounts of information should spew out or be made available within the deep substrates of space. Scientific America (November 2004) featured an article on Black Hole Computers, and states:  “Stephen Hawking was Wrong.  Matter goes in.  Answers come out.”  The loss of entropy which occurs as matter and energy are absorbed into a black hole is compensated for by an increase of the entropy of the information—thus, answers come out—like the 0 and 1 sequences illustrated here as emerging from a mini black hole processor.

{% include image.html path="images/universe_capacity/6.png" path-detail="images/universe_capacity/6.png" alt="" %}

Thermodynamic entropy, in contrast, depends on the states of all the billions of atoms (and their roaming electrons) that make up each transistor. As miniaturization brings closer the day when each atom will store one bit of information for us, the useful Shannon entropy of the state-of-the-art microchip will edge closer in magnitude to its material's thermodynamic entropy. When the two entropies are calculated for the same degrees of freedom, they are equal.

What are the ultimate degrees of freedom? Atoms, after all, are made of electrons and nuclei, nuclei are agglomerations of protons and neutrons, and those in turn are composed of quarks. Many physicists today consider electrons and quarks to be excitations of superstrings, which they hypothesize to be the most fundamental entities. But the vicissitudes of a century of revelations in physics warn us not to be dogmatic. There could be more levels of structure in our universe than are dreamt of in today's physics.

If we measure full information of a this example above as where the molecules of air is located and what speed every molecule has inside the sphere on a specific point in the timeline, we either increasing 2 times (except the tubule) the size of the sphere or pressure consequently we are increasing 2 times the entropy of the sphere which accordingly increases the full information of the system.

Dr. Bekenstein argues that there seems “to be no limits to how densely information can be packed-and that our universe might be like a giant hologram.“ 
In Bekenstein's model, the more we penetrate into the heart of being, vast amounts of information might be contained within the seeming emptiness and there might be such complex inner worlds and black hole dynamics.
Black holes have both mass and rotational or spin properties, are capable of being electrically charged, and they could function as the ultimate mini-computers, processing immense amount of information at Planckian levels. Everything computes in the new science of information theory, including black holes. 

This means that we got the maximum amount of the material density and, сonsequently, the maximum amount of entropy which means that we got the maximum amount of information which can lie on a specific point of area.
Therefore you cannot collect more information (of particles and their own relationship between each other) on a specific point of the area rather than a black hole.


> Composite bodies made of multidimensional structures called branes, which arise in string theory.  Information falling into the black hole is stored in waves in the branes and can eventually leak out.  …  Mathur … and his collaborators modelled a black hole as “a giant tangle of strings.” This “fuzzyball” acts as a repository of the information carried by things that fall into the black hole.  It emits radiation that reflects this information.
> Lloyd and Ng report the work of Horowitz suggesting that this information has another “escape hatch” out of a black hole — that of ‘entanglement,’ whereby the properties of two systems inside and outside of the black hole remain correlated across spacetime: 
> Entanglement enables teleportation, in which information is transferred from one particle to another with such fidelity that the particle has effectively been beamed from one location to another at up to the speed of light. …  The annihilation of the infalling photon acts as a measurement, transferring the information contained in the matter to the outgoing Hawking radiation. 

{% include image.html path="images/universe_capacity/qtp.jpg" path-detail="images/universe_capacity/qtp.jpg" alt="" %}

According to Hawking radiation hypothetical process

{% include image.html path="images/universe_capacity/2.png" path-detail="images/universe_capacity/2.png" alt="" %}

 … we can measure the entropy of the black hole

{% include image.html path="images/universe_capacity/3.png" path-detail="images/universe_capacity/3.png" alt="" %}

For instance: Entropy density of a Reissner-Nordström black hole where

`c = G = kRN = 1, M = 1`

{% include image.html path="images/universe_capacity/4.png" path-detail="images/universe_capacity/4.png" alt="" %}

This means that the maximum amount of entropy which the black hole can handle is proportional to their event horizon area, not the size of the black hole. 
Also, from the Steven Hawking's “The four laws of black hole mechanics“ publication we can realise that using the thermodynamic relationship between energy, temperature and entropy, Hawking was able to confirm Bekenstein's conjecture and fix the constant of proportionality at 1/4: 

{% include image.html path="images/universe_capacity/5.png" path-detail="images/universe_capacity/5.png" alt="" %}

Which means:
Entropy of the black hole `S` is quantitatively equal to their event horizon area `A` expressed in the small units of the planck length `l^2`.

Therefore the maximum amount of information which can theoretically contains inside the some point in space equals to the area of the sphere expressed by planck triangles it represented by.
In total: The maximum amount of information in space is proportional to the surface area of the sphere it's represented by using the planck length.

Paradoxically, this physics of black holes suggests that as a physical system of matter and energy collapses down to the level of Planck units at `10e-33cm` and beyond, the amount of information potentially contained within a volume of space becomes huge — if there are indeed such mini-black hole dynamics.   
Bekenstein considers the amount of information contained within 'a Planck area' which is the square of two Planck lengths of `10e-33cm` — or `10e-66cm^2`.
At Planck’s level, at zero point levels in the quantum vacuum or aether, the information content and capacity is potentially huge.  
Bekenstein states: 
> The entropy of a black hole one centimetre in diameter would be about 10e+66 bits, roughly equal to the thermodynamic entropy of a cube of water 10 billion kilometres on a side.

But actually there much less, because we are not living in the black hole. 
There it is possible to make a retreat and talk about the holographic theory. 
Relying on which we are all holographic events projection which exist somewhere on the theoretical boundary of the universe, not the material units. 

Briefly: We are not living in the black hole.

## And then
______
To find the diameter of the observable universe and according to formula find the surface area of the sphere and divide it by the planck area (`~1.13107e-70m^2`). 
The WMAP Observatory team states that our metagalaxy is `13.75±0.13e+9` years old. 
Next calculations will take that the universe expansion has a constant speed (which is not) and does not exceeds the speed of light.

{% include image.html path="images/universe_capacity/era.jpg" path-detail="images/universe_capacity/era.jpg" alt="" %}

And ofcourse it must be mentioned that we have a deal between 2 contradictions. 
On the one side we had that sphere with the ideal gas inside and entropy of this sphere was proportional to their volume. 
On the other side we had a black hole where the material was a maximally gravitationally connected between it's molecules. 
There entropy was proportionally depended by their surface area. In our universe there is the gravity but it's not so strong than black hole's which means that we have something between.

According that the speed of light equals to `299,792,458m/s` and the age of the universe is nearly equal `13.75±0.13e+9` years we can find the area of the surface of our universe using the simple formula: `S = πR^2`

{% include image.html path="images/universe_capacity/calc1.png" path-detail="images/universe_capacity/calc1.png" alt="" %}

Here I've calculated thee universe capacity at a certain moment in time. 
I mentioned that I need to take something in between and this means that I need to calculate the maximum information capacity of the universe before Big Bang (precisely I'll take the Planck era as starting point, not singularity) and find an average (where singularity epoch is 0)

{% include image.html path="images/universe_capacity/calc2.png" path-detail="images/universe_capacity/calc2.png" alt="" %}

And finally the average
{% include image.html path="images/universe_capacity/calc3.png" path-detail="images/universe_capacity/calc3.png" alt="" %}

Which is equals to
{% include image.html path="images/universe_capacity/calc4.png" path-detail="images/universe_capacity/calc4.png" alt="" %}

Thus, in last `13.75±0.13e+9` years the universe maximum information capacity was `4.7194412e+107`. 
Then calculate the full amount information of the universe existance from its born to death in the distant future (`~10e+10000 years`).

{% include image.html path="images/universe_capacity/calc5.png" path-detail="images/universe_capacity/calc5.png" alt="" %}

Which is equal to:
{% include image.html path="images/universe_capacity/calc6.png" path-detail="images/universe_capacity/calc6.png" alt="" %}

Or maybe I'm just being retarded.

### See also
* “[Area Dependence of Scalar Field
Entanglement Entropy](https://f.lewd.se/IRF4kb.pdf)“ Tim Bakker
* [UC Berkeley's Raphael Bousso gives an introductory lecture on the holographic principle \[Video\]](http://www.uctv.tv/shows/The-World-as-a-Hologram-11140)
* “[What is the Ultimate Fate of the Universe?](https://map.gsfc.nasa.gov/universe/uni_fate.html)“
* “[The Road to Reality](http://chaosbook.org/library/Penr04.pdf)“ Roger Penrose

Special thanks to Christopher Holmes 
